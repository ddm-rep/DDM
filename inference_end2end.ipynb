{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Video writer\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# Repo imports\n",
    "import sys\n",
    "sys.path.append('/workspace')\n",
    "sys.path.append('/workspace/planner')\n",
    "\n",
    "from planner.modules.config_planner import ConfigPlanner\n",
    "from planner.modules.transformer import LatentPlanner\n",
    "\n",
    "from models.diffusion import AsymmetricUNet\n",
    "from models.scheduler import NoiseScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import ndimage\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def to_uint8_img(x_bchw: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"x in [-1,1], shape [B,3,H,W] -> uint8 [H,W,3]\"\"\"\n",
    "    x = x_bchw.detach().float().clamp(-1, 1)\n",
    "    x = (x + 1) * 0.5\n",
    "    x = (x * 255.0).round().to(torch.uint8)\n",
    "    x = x[0].permute(1, 2, 0).cpu().numpy()\n",
    "    return x\n",
    "\n",
    "\n",
    "def extract_ball_position(frame: np.ndarray, threshold: float = 0.7) -> Optional[Tuple[float, float]]:\n",
    "    if len(frame.shape) == 3:\n",
    "        gray = np.mean(frame, axis=2).astype(np.float32) / 255.0\n",
    "    else:\n",
    "        gray = frame.astype(np.float32)\n",
    "        if gray.max() > 1.0:\n",
    "            gray = gray / 255.0\n",
    "    \n",
    "    mask = gray > threshold\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        max_idx = np.unravel_index(np.argmax(gray), gray.shape)\n",
    "        return (float(max_idx[0]), float(max_idx[1]))\n",
    "    \n",
    "    y_coords, x_coords = np.where(mask)\n",
    "    if len(y_coords) == 0:\n",
    "        return None\n",
    "    \n",
    "    weights = gray[y_coords, x_coords]\n",
    "    total_weight = np.sum(weights)\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        y_center = np.average(y_coords, weights=weights)\n",
    "        x_center = np.average(x_coords, weights=weights)\n",
    "    else:\n",
    "        y_center = np.mean(y_coords)\n",
    "        x_center = np.mean(x_coords)\n",
    "    \n",
    "    return (float(y_center), float(x_center))\n",
    "\n",
    "\n",
    "def extract_trajectory(video: torch.Tensor, threshold: float = 0.7, debug: bool = False) -> List[Tuple[float, float]]:\n",
    "    def to_uint8(tensor):\n",
    "        tensor = tensor.detach().float().clamp(-1, 1)\n",
    "        tensor = (tensor + 1) * 0.5\n",
    "        tensor = (tensor * 255.0).round().to(torch.uint8)\n",
    "        return tensor\n",
    "    \n",
    "    if video.ndim == 5:  # [B, T, C, H, W]\n",
    "        video = video[0]\n",
    "    \n",
    "    T = video.shape[0]\n",
    "    trajectory = []\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        frame_tensor = to_uint8(video[t])\n",
    "        frame = frame_tensor.permute(1, 2, 0).cpu().numpy()  # [H, W, 3]\n",
    "        pos = extract_ball_position(frame, threshold)\n",
    "        if pos is not None:\n",
    "            trajectory.append(pos)\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "            if len(trajectory) > 0:\n",
    "                trajectory.append(trajectory[-1])\n",
    "                if debug:\n",
    "                    print(f\"  Frame {t}: pos=None, using previous position {trajectory[-1]}\")\n",
    "            else:\n",
    "                trajectory.append((0.0, 0.0))\n",
    "                if debug:\n",
    "                    print(f\"  Frame {t}: pos=None, using default (0, 0)\")\n",
    "    \n",
    "    if debug and invalid_count > 0:\n",
    "        print(f\"  Warning: {invalid_count}/{T} frames had invalid positions\")\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def get_resolution_for_timestep(t: int, num_timesteps: int, high_res: int, latent_res: int, k_step: int) -> int:\n",
    "    ratio = t / (num_timesteps - 1)\n",
    "    size_float = high_res - ratio * (high_res - latent_res)\n",
    "    size_int = int(round(size_float / k_step) * k_step)\n",
    "    size_int = max(latent_res, min(high_res, size_int))\n",
    "    return size_int\n",
    "\n",
    "\n",
    "def planner_generate(planner: LatentPlanner, cond_btchw: torch.Tensor, total_T: int, show_progress: bool = True) -> torch.Tensor:\n",
    "    planner.eval()\n",
    "    B, k, C, H, W = cond_btchw.shape\n",
    "    frames = [cond_btchw[:, i] for i in range(k)]\n",
    "\n",
    "    iterator = range(total_T - k)\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, desc=\"Generating frames\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in iterator:\n",
    "            seq = torch.stack(frames, dim=1)  # [B, t, C,H,W]\n",
    "            attn = torch.ones(B, seq.shape[1], device=seq.device)\n",
    "            pred = planner(seq, attn_mask=attn)  # [B, t+1, C,H,W]\n",
    "            next_frame = pred[:, seq.shape[1]]   # prediction for f_t\n",
    "            frames.append(next_frame)\n",
    "\n",
    "    out = torch.stack(frames, dim=1)\n",
    "    return out\n",
    "\n",
    "# Backward compatibility\n",
    "def planner_generate_from_5(planner: LatentPlanner, cond_5_btchw: torch.Tensor, total_T: int) -> torch.Tensor:\n",
    "    \"\"\"Wrapper for backward compatibility\"\"\"\n",
    "    return planner_generate(planner, cond_5_btchw, total_T, show_progress=True)\n",
    "\n",
    "\n",
    "def refiner_refine_sequence(\n",
    "    refiner: AsymmetricUNet,\n",
    "    scheduler: NoiseScheduler,\n",
    "    lowres_btchw: torch.Tensor,\n",
    "    high_res: int = 128,\n",
    "    latent_res: int = 32,\n",
    "    k_step: int = 1,\n",
    "    t_start_frac: float = 0.1,\n",
    "    batch_frames: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    lowres_btchw: [B,T,3,latent_res,latent_res] in [-1,1]\n",
    "    return: [B,T,3,high_res,high_res] in [-1,1]\n",
    "\n",
    "    Note: This uses x0-pred sampling (scheduler.step_x0).\n",
    "    \n",
    "    Args:\n",
    "        batch_frames: If True, process all frames in parallel (B*T batch). Much faster but uses more memory.\n",
    "    \"\"\"\n",
    "    refiner.eval()\n",
    "    B, T, C, H, W = lowres_btchw.shape\n",
    "    assert H == latent_res and W == latent_res\n",
    "\n",
    "    t_start = int((scheduler.num_timesteps - 1) * float(t_start_frac))\n",
    "    \n",
    "    print(f\"  [Refiner Debug] num_timesteps={scheduler.num_timesteps}, t_start_frac={t_start_frac}\")\n",
    "    print(f\"  [Refiner Debug] t_start={t_start}, will run from {t_start} to 0 (total {t_start + 1} steps)\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if batch_frames:\n",
    "            out_frames_list = []\n",
    "            \n",
    "            pbar_videos = tqdm(range(B), desc=\"Refining videos\", total=B, miniters=1, mininterval=1.0)\n",
    "            \n",
    "            for b_idx in pbar_videos:\n",
    "                latent_video = lowres_btchw[b_idx]  # [T, 3, 32, 32]\n",
    "                \n",
    "                noise = torch.randn_like(latent_video)\n",
    "                timesteps = torch.full((T,), t_start, device=latent_video.device, dtype=torch.long)\n",
    "                curr = scheduler.add_noise(latent_video, noise, timesteps)  # [T, 3, 32, 32]\n",
    "\n",
    "                num_steps = t_start + 1\n",
    "                pbar_diffusion = tqdm(\n",
    "                    range(t_start, -1, -1), \n",
    "                    desc=f\"Video {b_idx+1}/{B} diffusion\", \n",
    "                    total=num_steps,\n",
    "                    leave=False,\n",
    "                    miniters=max(1, num_steps // 20),  \n",
    "                    mininterval=0.5\n",
    "                )\n",
    "                \n",
    "                for t in pbar_diffusion:\n",
    "                    t_batch = torch.full((T,), t, device=latent_video.device, dtype=torch.long)\n",
    "                    target_res_t = get_resolution_for_timestep(t, scheduler.num_timesteps, high_res, latent_res, k_step)\n",
    "\n",
    "                    # Ensure curr is at target_res_t\n",
    "                    curr_h, curr_w = curr.shape[-2:]\n",
    "                    if curr_h != target_res_t or curr_w != target_res_t:\n",
    "                        curr = F.interpolate(curr, size=(target_res_t, target_res_t), mode='bilinear', align_corners=False)\n",
    "                        curr_h, curr_w = target_res_t, target_res_t\n",
    "\n",
    "                    # Predict x0 at high res (batch all T frames)\n",
    "                    pred_x0_high = refiner(curr, t_batch, target_shape=(high_res, high_res))  # [T, 3, 128, 128]\n",
    "\n",
    "                    # Downsample x0 to curr's actual resolution\n",
    "                    pred_x0_curr = F.interpolate(pred_x0_high, size=(curr_h, curr_w), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    # Ensure exact shape match\n",
    "                    assert pred_x0_curr.shape == curr.shape, \\\n",
    "                        f\"Shape mismatch: pred_x0_curr {pred_x0_curr.shape}, curr {curr.shape}\"\n",
    "                    \n",
    "                    # Use scalar timestep for scheduler\n",
    "                    t_val = int(t)\n",
    "                    prev = scheduler.step_x0(pred_x0_curr, t_val, curr)\n",
    "\n",
    "                    if t > 0:\n",
    "                        next_res = get_resolution_for_timestep(t - 1, scheduler.num_timesteps, high_res, latent_res, k_step)\n",
    "                        # Ensure prev is at next_res before assigning to curr\n",
    "                        if prev.shape[-1] != next_res or prev.shape[-2] != next_res:\n",
    "                            curr = F.interpolate(prev, size=(next_res, next_res), mode='bilinear', align_corners=False)\n",
    "                        else:\n",
    "                            curr = prev\n",
    "                    else:\n",
    "                        curr = prev\n",
    "                \n",
    "                pbar_diffusion.close()\n",
    "\n",
    "                # Final upscale to high_res\n",
    "                if curr.shape[-1] != high_res or curr.shape[-2] != high_res:\n",
    "                    curr = F.interpolate(curr, size=(high_res, high_res), mode='bilinear', align_corners=False)\n",
    "\n",
    "                out_frames_list.append(curr)  # [T, 3, 128, 128]\n",
    "                pbar_videos.update(1)\n",
    "            \n",
    "            pbar_videos.close()\n",
    "            \n",
    "            # Stack all videos: [B, T, C, H, W]\n",
    "            out = torch.stack(out_frames_list, dim=0)\n",
    "            \n",
    "        else:\n",
    "            # Sequential mode (original): process frame by frame\n",
    "            out_frames = []\n",
    "            num_steps = t_start + 1\n",
    "            \n",
    "            pbar_frames = tqdm(\n",
    "                range(T), \n",
    "                desc=\"Refining frames\", \n",
    "                total=T,\n",
    "                miniters=1,\n",
    "                mininterval=1.0\n",
    "            )\n",
    "            \n",
    "            for i in pbar_frames:\n",
    "                latent_img = lowres_btchw[:, i]  # [B,3,32,32]\n",
    "\n",
    "                # Start from noisy latent at timestep t_start\n",
    "                noise = torch.randn_like(latent_img)\n",
    "                timesteps = torch.full((B,), t_start, device=latent_img.device, dtype=torch.long)\n",
    "                curr = scheduler.add_noise(latent_img, noise, timesteps)\n",
    "\n",
    "                pbar_diffusion = tqdm(\n",
    "                    range(t_start, -1, -1),\n",
    "                    desc=f\"Frame {i+1}/{T} diffusion\",\n",
    "                    total=num_steps,\n",
    "                    leave=False,\n",
    "                    miniters=max(1, num_steps // 20),\n",
    "                    mininterval=0.5\n",
    "                )\n",
    "                \n",
    "                for t in pbar_diffusion:\n",
    "                    t_batch = torch.full((B,), t, device=latent_img.device, dtype=torch.long)\n",
    "                    target_res_t = get_resolution_for_timestep(t, scheduler.num_timesteps, high_res, latent_res, k_step)\n",
    "\n",
    "                    if curr.shape[-1] != target_res_t:\n",
    "                        curr = F.interpolate(curr, size=(target_res_t, target_res_t), mode='bilinear', align_corners=False)\n",
    "\n",
    "                    # Predict x0 at high res\n",
    "                    pred_x0_high = refiner(curr, t_batch, target_shape=(high_res, high_res))\n",
    "\n",
    "                    # Downsample x0 to current resolution for step\n",
    "                    pred_x0_curr = F.interpolate(pred_x0_high, size=(target_res_t, target_res_t), mode='bilinear', align_corners=False)\n",
    "\n",
    "                    prev = scheduler.step_x0(pred_x0_curr, t_batch, curr)\n",
    "\n",
    "                    if t > 0:\n",
    "                        next_res = get_resolution_for_timestep(t - 1, scheduler.num_timesteps, high_res, latent_res, k_step)\n",
    "                        if prev.shape[-1] != next_res:\n",
    "                            curr = F.interpolate(prev, size=(next_res, next_res), mode='bilinear', align_corners=False)\n",
    "                        else:\n",
    "                            curr = prev\n",
    "                    else:\n",
    "                        curr = prev\n",
    "                \n",
    "                pbar_diffusion.close()\n",
    "\n",
    "                # Final upscale to high_res\n",
    "                if curr.shape[-1] != high_res:\n",
    "                    curr = F.interpolate(curr, size=(high_res, high_res), mode='bilinear', align_corners=False)\n",
    "\n",
    "                out_frames.append(curr)\n",
    "                pbar_frames.update(1)\n",
    "            \n",
    "            pbar_frames.close()\n",
    "            \n",
    "            out = torch.stack(out_frames, dim=1)  # [B,T,3,H,W]\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Config (EDIT ONLY THIS CELL) ======\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Checkpoints ---\n",
    "PLANNER_CKPT = '/workspace/planner/experiments/20260123_073354_planner_32_large/checkpoints/model_epoch_1000.pth'   # e.g. '/workspace/planner/experiments/XXXX/checkpoints/model_epoch_500.pth'\n",
    "REFINER_CKPT = '/workspace/experiments/20260120_100656_obj_weight_5/checkpoints/model_epoch_30.pt'   # e.g. '/workspace/experiments/XXXX/checkpoints/model_epoch_500.pt'\n",
    "\n",
    "# --- Data (FORCED: 2nd-stage preprocessed low-res) ---\n",
    "LOWRES_ROOT = '/workspace/data/processed_32'  # must be 32x32 .pt\n",
    "SPLIT = 'val'\n",
    "OUT_DIR = '/workspace/inferences/012345'\n",
    "\n",
    "# Single-sample mode\n",
    "SAMPLE_INDEX = 0  # which video file to use from LOWRES_ROOT/SPLIT\n",
    "\n",
    "# Multi-sample mode (batch inference)\n",
    "# - set to a list like [0,1,2,3] to run multiple videos in one batch\n",
    "# - if empty/None, falls back to [SAMPLE_INDEX]\n",
    "# - Example: SAMPLE_INDICES = [0, 1, 2, 3]  # Process 4 samples in parallel\n",
    "SAMPLE_INDICES = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# How many samples to render to mp4 (one mp4 per sample)\n",
    "# - None: use all samples in SAMPLE_INDICES (or just SAMPLE_INDEX if SAMPLE_INDICES is empty)\n",
    "# - e.g. 4: only render first 4 samples\n",
    "NUM_SAMPLES_TO_RENDER = None  # e.g. 4, or None to use all in SAMPLE_INDICES\n",
    "\n",
    "# --- Sequence lengths ---\n",
    "COND_FRAMES_LIST = [1, 3, 5, 10, 15]\n",
    "COND_FRAMES = 5 \n",
    "TOTAL_FRAMES = 100   # total frames to generate/refine (clipped by sample length)\n",
    "T_RENDER = 100        # how many frames to run the refiner + write mp4 (cost control)\n",
    "\n",
    "# Figure 1: Qualitative Video Generation\n",
    "FIGURE1_TIMESTEPS = [10, 30, 60, 100]\n",
    "FIGURE1_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "\n",
    "# Figure 2: Trajectory Comparison\n",
    "FIGURE2_NUM_SAMPLES = 4\n",
    "FIGURE2_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "\n",
    "# Figure 3: Conditioning Effect Analysis\n",
    "FIGURE3_COND_FRAMES_LIST = COND_FRAMES_LIST\n",
    "FIGURE3_SEQUENCE_INDEX = 0\n",
    "FIGURE3_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "\n",
    "# Figure 4: Temporal Consistency Analysis\n",
    "FIGURE4_SEQUENCE_INDEX = 0\n",
    "FIGURE4_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "\n",
    "DATASET_FIG_NUM_SAMPLES_GRID = 12\n",
    "DATASET_FIG_NUM_SEQUENCES_TIMELINE = 3\n",
    "DATASET_FIG_TIMESTEPS = [0, 20, 40, 60, 80, 100]\n",
    "DATASET_FIG_NUM_TRAJECTORIES = 4\n",
    "DATASET_FIG_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "\n",
    "\n",
    "# --- Resolutions / schedule ---\n",
    "LATENT_RES = 32\n",
    "HIGH_RES = 128\n",
    "K_STEP = 1\n",
    "\n",
    "# --- Refiner sampling strength ---\n",
    "# 0.0~1.0. Lower => less destruction (more refinement). Recommended: 0.05~0.2\n",
    "T_START_FRAC = 0.1\n",
    "\n",
    "# --- Refiner batch processing ---\n",
    "# If True, process all frames in parallel (B*T batch). Much faster but uses more GPU memory.\n",
    "# If False, process frames sequentially (slower but uses less memory).\n",
    "REFINER_BATCH_FRAMES = True\n",
    "\n",
    "# --- Refiner model architecture (must match the checkpoint you load) ---\n",
    "REF_MODEL_CHANNELS = 128\n",
    "REF_CHANNEL_MULT = (1, 2, 4, 8)\n",
    "REF_NUM_RES_BLOCKS = 2\n",
    "\n",
    "# --- Outputs ---\n",
    "OUT_MP4 = os.path.join(OUT_DIR, 'out_end2end.mp4')\n",
    "FPS = 10\n",
    "\n",
    "COND_PREVIEW_PNG = os.path.join(OUT_DIR, 'cond_5.png')\n",
    "LOWRES_PREVIEW_PNG = os.path.join(OUT_DIR, 'lowres_preview.png')\n",
    "HIGHRES_PREVIEW_PNG = os.path.join(OUT_DIR, 'highres_preview.png')\n",
    "\n",
    "print('DEVICE:', DEVICE)\n",
    "print('LOWRES_ROOT:', LOWRES_ROOT)\n",
    "print('SPLIT:', SPLIT, 'SAMPLE_INDEX:', SAMPLE_INDEX)\n",
    "print('OUT_MP4:', OUT_MP4)\n",
    "print('T_RENDER:', T_RENDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('COND_FRAMES_LIST:', COND_FRAMES_LIST)\n",
    "print('Total SAMPLE_INDICES :', len(SAMPLE_INDICES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Load low-res GT samples (for conditioning) ======\n",
    "pt_files = sorted(glob.glob(os.path.join(LOWRES_ROOT, SPLIT, '*.pt')))\n",
    "assert len(pt_files) > 0, f'No .pt files found in {LOWRES_ROOT}/{SPLIT}'\n",
    "\n",
    "# Determine which samples to load\n",
    "if SAMPLE_INDICES and len(SAMPLE_INDICES) > 0:\n",
    "    indices_to_load = [int(i) % len(pt_files) for i in SAMPLE_INDICES]\n",
    "else:\n",
    "    indices_to_load = [int(SAMPLE_INDEX) % len(pt_files)]\n",
    "\n",
    "if NUM_SAMPLES_TO_RENDER is not None:\n",
    "    indices_to_load = indices_to_load[:int(NUM_SAMPLES_TO_RENDER)]\n",
    "\n",
    "print(f'Loading {len(indices_to_load)} samples: {indices_to_load}')\n",
    "\n",
    "# Load all samples\n",
    "conds_list = []\n",
    "seqs_list = []\n",
    "sample_paths = []\n",
    "\n",
    "for idx in indices_to_load:\n",
    "    sample_path = pt_files[idx]\n",
    "    sample_paths.append(sample_path)\n",
    "    print(f'  [{idx}] {os.path.basename(sample_path)}')\n",
    "    \n",
    "    x = torch.load(sample_path, map_location='cpu')  # uint8 [T,3,32,32]\n",
    "    assert torch.is_tensor(x) and x.ndim == 4 and x.shape[1] == 3, f\"{sample_path}: {x.shape}\"\n",
    "    \n",
    "    # normalize to [-1,1]\n",
    "    seq = (x.float() / 255.0 - 0.5) * 2.0  # [T,3,32,32]\n",
    "    \n",
    "    # pick first TOTAL_FRAMES (or shorter)\n",
    "    T_avail = seq.shape[0]\n",
    "    T_use = min(int(TOTAL_FRAMES), int(T_avail))\n",
    "    seq = seq[:T_use]\n",
    "    \n",
    "    max_cond_frames = max(COND_FRAMES_LIST) if isinstance(COND_FRAMES_LIST, list) else COND_FRAMES\n",
    "    cond = seq[:int(max_cond_frames)]\n",
    "    conds_list.append(cond)\n",
    "    seqs_list.append(seq)\n",
    "\n",
    "if isinstance(COND_FRAMES_LIST, list) and len(COND_FRAMES_LIST) > 0:\n",
    "    cond_frames_list_to_process = COND_FRAMES_LIST\n",
    "else:\n",
    "    cond_frames_list_to_process = [COND_FRAMES] if isinstance(COND_FRAMES, int) else [5]\n",
    "\n",
    "print(f'Will process cond_frames: {cond_frames_list_to_process}')\n",
    "\n",
    "cond_frames_to_use = cond_frames_list_to_process[0]\n",
    "cond_batch = torch.stack([cond[:int(cond_frames_to_use)] for cond in conds_list], dim=0).to(DEVICE)\n",
    "print(f'Preview cond_batch: {cond_batch.shape} (using cond_frames={cond_frames_to_use})')\n",
    "\n",
    "# Visualize first sample's condition frames\n",
    "imgs = [to_uint8_img(cond_batch[0:1, i]) for i in range(int(cond_frames_to_use))]\n",
    "Image.fromarray(np.hstack(imgs)).save(COND_PREVIEW_PNG)\n",
    "print('Saved', COND_PREVIEW_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Load Planner ======\n",
    "config = ConfigPlanner()\n",
    "\n",
    "# Force settings consistent with low-res\n",
    "config.target_res = LATENT_RES\n",
    "config.max_seq_len = TOTAL_FRAMES\n",
    "config.latent_dim = 3 * LATENT_RES * LATENT_RES\n",
    "\n",
    "planner = LatentPlanner(config).to(DEVICE)\n",
    "\n",
    "assert PLANNER_CKPT, 'Set PLANNER_CKPT path in the config cell'\n",
    "state = torch.load(PLANNER_CKPT, map_location='cpu')\n",
    "# train_planner.py saves state_dict directly\n",
    "planner.load_state_dict(state, strict=False)\n",
    "planner.eval()\n",
    "\n",
    "print('Loaded planner:', PLANNER_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Load Refiner ======\n",
    "assert REFINER_CKPT, 'Set REFINER_CKPT path in the config cell'\n",
    "\n",
    "refiner = AsymmetricUNet(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    model_channels=int(REF_MODEL_CHANNELS),\n",
    "    channel_mult=tuple(REF_CHANNEL_MULT),\n",
    "    num_res_blocks=int(REF_NUM_RES_BLOCKS),\n",
    ").to(DEVICE)\n",
    "\n",
    "ref_state = torch.load(REFINER_CKPT, map_location='cpu')\n",
    "refiner.load_state_dict(ref_state, strict=False)\n",
    "refiner.eval()\n",
    "\n",
    "noise_scheduler = NoiseScheduler(device=DEVICE)\n",
    "\n",
    "print('Loaded refiner:', REFINER_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Model summaries (Planner / Refiner) ======\n",
    "\n",
    "params_sum = 0\n",
    "def count_params(m: torch.nn.Module):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "print(\"\\n=== LatentPlanner Model Summary ===\")\n",
    "if 'planner' not in globals():\n",
    "    print(\"[WARN] planner is not loaded yet. Run the 'Load Planner' cell first.\")\n",
    "else:\n",
    "    total, trainable = count_params(planner)\n",
    "    print(f\"Params: {total/1e6:.3f}M (trainable {trainable/1e6:.3f}M)\")\n",
    "    params_sum += total\n",
    "\n",
    "print(\"\\n=== AsymmetricUNet (Refiner) Model Summary ===\")\n",
    "if 'refiner' not in globals():\n",
    "    print(\"[WARN] refiner is not loaded yet. Run the 'Load Refiner' cell first.\")\n",
    "else:\n",
    "    total, trainable = count_params(refiner)\n",
    "    print(f\"Params: {total/1e6:.3f}M (trainable {trainable/1e6:.3f}M)\")\n",
    "    params_sum += total\n",
    "\n",
    "print(f\"\\nTotal params: {params_sum/1e6:.3f}M\")\n",
    "print(f\"\\nTotal params: {params_sum/1e9:.3f}B\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Planner generation (LLM-style, batch processing) ======\n",
    "T_uses = [min(int(TOTAL_FRAMES), seq.shape[0]) for seq in seqs_list]\n",
    "T_use_batch = min(T_uses)  # Use minimum length for batch consistency\n",
    "\n",
    "lowres_gen_batches = {}  # {cond_frames: lowres_gen_batch}\n",
    "highres_gen_batches = {}  # {cond_frames: highres_gen_batch}\n",
    "\n",
    "for cond_frames_val in cond_frames_list_to_process:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Processing cond_frames={cond_frames_val}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    cond_batch_cf = torch.stack([cond[:int(cond_frames_val)] for cond in conds_list], dim=0).to(DEVICE)\n",
    "    print(f'cond_batch: {cond_batch_cf.shape} (using cond_frames={cond_frames_val})')\n",
    "    \n",
    "    lowres_gen_batch = planner_generate(planner, cond_btchw=cond_batch_cf, total_T=T_use_batch, show_progress=True)\n",
    "    print(f'lowres_gen_batch: {lowres_gen_batch.shape}')  # [B, T, 3, 32, 32]\n",
    "    \n",
    "    lowres_gen_batches[cond_frames_val] = lowres_gen_batch\n",
    "    \n",
    "    if cond_frames_val == cond_frames_list_to_process[0]:\n",
    "        preview = [to_uint8_img(lowres_gen_batch[0:1, i]) for i in range(min(10, lowres_gen_batch.shape[1]))]\n",
    "        Image.fromarray(np.hstack(preview)).save(LOWRES_PREVIEW_PNG)\n",
    "        print('Saved', LOWRES_PREVIEW_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Refine low-res sequences to high-res (batch processing) ======\n",
    "# NOTE: this is expensive. Control cost via T_RENDER in the config cell.\n",
    "T_RENDER_EFF = min(int(T_RENDER), T_use_batch)\n",
    "\n",
    "for cond_frames_val in cond_frames_list_to_process:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Refining cond_frames={cond_frames_val}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    lowres_gen_batch = lowres_gen_batches[cond_frames_val]\n",
    "    lowres_subset_batch = lowres_gen_batch[:, :T_RENDER_EFF]\n",
    "    \n",
    "    print(f'Refining {lowres_subset_batch.shape[0]} samples, {T_RENDER_EFF} frames each...')\n",
    "    \n",
    "    highres_batch = refiner_refine_sequence(\n",
    "        refiner=refiner,\n",
    "        scheduler=noise_scheduler,\n",
    "        lowres_btchw=lowres_subset_batch,\n",
    "        high_res=int(HIGH_RES),\n",
    "        latent_res=int(LATENT_RES),\n",
    "        k_step=int(K_STEP),\n",
    "        t_start_frac=float(T_START_FRAC),\n",
    "        batch_frames=bool(REFINER_BATCH_FRAMES),\n",
    "    )\n",
    "    print(f'highres_batch: {highres_batch.shape}')  # [B, T, 3, 128, 128]\n",
    "    \n",
    "    highres_gen_batches[cond_frames_val] = highres_batch\n",
    "    \n",
    "    if cond_frames_val == cond_frames_list_to_process[0]:\n",
    "        preview_hr = [to_uint8_img(highres_batch[0:1, i]) for i in range(min(10, highres_batch.shape[1]))]\n",
    "        Image.fromarray(np.hstack(preview_hr)).save(HIGHRES_PREVIEW_PNG)\n",
    "        print('Saved', HIGHRES_PREVIEW_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highres_batch = highres_gen_batches[cond_frames_list_to_process[0]]\n",
    "lowres_gen_batch = lowres_gen_batches[cond_frames_list_to_process[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "B = highres_batch.shape[0]\n",
    "T = highres_batch.shape[1]\n",
    "\n",
    "for b_idx in range(B):\n",
    "    sample_name = os.path.splitext(os.path.basename(sample_paths[b_idx]))[0]\n",
    "    out_mp4_path = os.path.join(OUT_DIR, f'out_sample_{b_idx:02d}_{sample_name}.mp4')\n",
    "    \n",
    "    with imageio.get_writer(out_mp4_path, fps=FPS, codec='libx264', quality=8) as w:\n",
    "        for t_idx in range(T):\n",
    "            frame = to_uint8_img(highres_batch[b_idx:b_idx+1, t_idx])\n",
    "            w.append_data(frame)\n",
    "    \n",
    "    print(f'[{b_idx+1}/{B}] Wrote: {out_mp4_path}')\n",
    "\n",
    "if B > 0:\n",
    "    with imageio.get_writer(OUT_MP4, fps=FPS, codec='libx264', quality=8) as w:\n",
    "        for t_idx in range(T):\n",
    "            frame = to_uint8_img(highres_batch[0:1, t_idx])\n",
    "            w.append_data(frame)\n",
    "    print(f'Also wrote default: {OUT_MP4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_HIGHRES_ROOT = '/workspace/data/processed'\n",
    "\n",
    "\n",
    "# Get high-res file list (same order as low-res)\n",
    "gt_pt_files = sorted(glob.glob(os.path.join(GT_HIGHRES_ROOT, SPLIT, '*.pt')))\n",
    "assert len(gt_pt_files) > 0, f'No .pt files found in {GT_HIGHRES_ROOT}/{SPLIT}'\n",
    "\n",
    "print(f'Loading GT high-res data using same indices: {indices_to_load}')\n",
    "\n",
    "gt_highres_list = []\n",
    "for idx, orig_idx in enumerate(indices_to_load):\n",
    "    # Use the same index from high-res file list\n",
    "    gt_path = gt_pt_files[orig_idx] if orig_idx < len(gt_pt_files) else None\n",
    "    \n",
    "    if gt_path and os.path.exists(gt_path):\n",
    "        try:\n",
    "            x_gt = torch.load(gt_path, map_location='cpu')  # uint8 [T, 3, H, W]\n",
    "            assert torch.is_tensor(x_gt) and x_gt.ndim == 4 and x_gt.shape[1] == 3, f\"{gt_path}: {x_gt.shape}\"\n",
    "            \n",
    "            # Normalize to [-1,1] and take same length as used\n",
    "            seq_gt = (x_gt.float() / 255.0 - 0.5) * 2.0  # [T, 3, H, W]\n",
    "            T_use_gt = min(seq_gt.shape[0], T_RENDER_EFF)\n",
    "            seq_gt = seq_gt[:T_use_gt]\n",
    "            \n",
    "            if seq_gt.shape[-1] != HIGH_RES or seq_gt.shape[-2] != HIGH_RES:\n",
    "                seq_gt = F.interpolate(\n",
    "                    seq_gt.permute(0, 1, 2, 3).reshape(-1, 3, seq_gt.shape[-2], seq_gt.shape[-1]),\n",
    "                    size=(HIGH_RES, HIGH_RES),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                ).reshape(T_use_gt, 3, HIGH_RES, HIGH_RES)\n",
    "                print(f'  [{idx}] (orig_idx={orig_idx}) Upscaled GT: {os.path.basename(gt_path)} ({T_use_gt} frames, {seq_gt.shape[-1]}x{seq_gt.shape[-2]})')\n",
    "            else:\n",
    "                print(f'  [{idx}] (orig_idx={orig_idx}) Loaded GT: {os.path.basename(gt_path)} ({T_use_gt} frames, {seq_gt.shape[-1]}x{seq_gt.shape[-2]})')\n",
    "            \n",
    "            gt_highres_list.append(seq_gt.to(DEVICE))\n",
    "        except Exception as e:\n",
    "            print(f'  [{idx}] (orig_idx={orig_idx}) Error loading GT: {e}')\n",
    "            seq_gt_fallback = F.interpolate(\n",
    "                seqs_list[idx][:T_RENDER_EFF].permute(0, 1, 2, 3).reshape(-1, 3, LATENT_RES, LATENT_RES).to(DEVICE),\n",
    "                size=(HIGH_RES, HIGH_RES),\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            ).reshape(T_RENDER_EFF, 3, HIGH_RES, HIGH_RES)\n",
    "            gt_highres_list.append(seq_gt_fallback)\n",
    "            print(f'  [{idx}] Using upscaled low-res as GT fallback')\n",
    "    else:\n",
    "        print(f'  [{idx}] (orig_idx={orig_idx}) WARNING: GT file not found, using upscaled low-res')\n",
    "        seq_gt_fallback = F.interpolate(\n",
    "            seqs_list[idx][:T_RENDER_EFF].permute(0, 1, 2, 3).reshape(-1, 3, LATENT_RES, LATENT_RES).to(DEVICE),\n",
    "            size=(HIGH_RES, HIGH_RES),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).reshape(T_RENDER_EFF, 3, HIGH_RES, HIGH_RES)\n",
    "        gt_highres_list.append(seq_gt_fallback)\n",
    "\n",
    "gt_highres_batch = torch.stack(gt_highres_list, dim=0)  # [B, T, 3, 128, 128]\n",
    "print(f'gt_highres_batch: {gt_highres_batch.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Ensure GT high-res batch has correct shape ======\n",
    "B = highres_batch.shape[0]\n",
    "T = highres_batch.shape[1]\n",
    "\n",
    "if gt_highres_batch.shape[-1] != HIGH_RES or gt_highres_batch.shape[-2] != HIGH_RES:\n",
    "    print(f'Upscaling GT from {gt_highres_batch.shape[-2]}x{gt_highres_batch.shape[-1]} to {HIGH_RES}x{HIGH_RES}')\n",
    "    gt_highres_batch = F.interpolate(\n",
    "        gt_highres_batch.reshape(B * T, 3, gt_highres_batch.shape[-2], gt_highres_batch.shape[-1]),\n",
    "        size=(HIGH_RES, HIGH_RES),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    ).reshape(B, T, 3, HIGH_RES, HIGH_RES)\n",
    "\n",
    "print(f'Final shapes - highres_batch: {highres_batch.shape}, gt_highres_batch: {gt_highres_batch.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Figure 1: Qualitative Video Generation ======\n",
    "\n",
    "os.makedirs(FIGURE1_OUT_DIR, exist_ok=True)\n",
    "\n",
    "B = highres_batch.shape[0]\n",
    "T = highres_batch.shape[1]\n",
    "\n",
    "timesteps_to_plot = [t for t in FIGURE1_TIMESTEPS if t <= T]\n",
    "timesteps_indices = [t - 1 for t in timesteps_to_plot]\n",
    "\n",
    "if len(timesteps_to_plot) == 0:\n",
    "    print(f\"Warning: No valid timesteps found. T={T}, requested={FIGURE1_TIMESTEPS}\")\n",
    "    timesteps_to_plot = [min(10, T), min(30, T), min(60, T), min(100, T)]\n",
    "    timesteps_to_plot = sorted(set(timesteps_to_plot))\n",
    "    timesteps_indices = [t - 1 for t in timesteps_to_plot]\n",
    "\n",
    "print(f\"Creating Figure 1 with timesteps: {timesteps_to_plot}\")\n",
    "\n",
    "for b_idx in range(B):\n",
    "    n_timesteps = len(timesteps_to_plot)\n",
    "    fig = plt.figure(figsize=(2 + 3*n_timesteps, 9))\n",
    "    gs = fig.add_gridspec(4, n_timesteps + 1, \n",
    "                          width_ratios=[0.3] + [1]*n_timesteps, \n",
    "                          height_ratios=[0.2, 1, 1, 1], \n",
    "                          hspace=0.15, wspace=0.1)\n",
    "    \n",
    "    for col_idx, t in enumerate(timesteps_to_plot):\n",
    "        ax = fig.add_subplot(gs[0, col_idx + 1])\n",
    "        ax.text(0.5, 0.5, f't={t}', fontsize=20, fontweight='bold', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    ax_label_gt = fig.add_subplot(gs[1, 0])\n",
    "    ax_label_gt.text(0.5, 0.5, 'GT', fontsize=20, fontweight='bold',\n",
    "                     ha='center', va='center', transform=ax_label_gt.transAxes)\n",
    "    ax_label_gt.axis('off')\n",
    "    \n",
    "    for col_idx, t_idx in enumerate(timesteps_indices):\n",
    "        ax = fig.add_subplot(gs[1, col_idx + 1])\n",
    "        gt_frame = to_uint8_img(gt_highres_batch[b_idx:b_idx+1, t_idx])\n",
    "        ax.imshow(gt_frame)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    ax_label_ours = fig.add_subplot(gs[2, 0])\n",
    "    ax_label_ours.text(0.5, 0.5, 'Ours', fontsize=20, fontweight='bold',\n",
    "                       ha='center', va='center', transform=ax_label_ours.transAxes)\n",
    "    ax_label_ours.axis('off')\n",
    "    \n",
    "    for col_idx, t_idx in enumerate(timesteps_indices):\n",
    "        ax = fig.add_subplot(gs[2, col_idx + 1])\n",
    "        pred_frame = to_uint8_img(highres_batch[b_idx:b_idx+1, t_idx])\n",
    "        ax.imshow(pred_frame)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    ax_label_overlay = fig.add_subplot(gs[3, 0])\n",
    "    ax_label_overlay.text(0.5, 0.5, 'Overlay', fontsize=20, fontweight='bold',\n",
    "                          ha='center', va='center', transform=ax_label_overlay.transAxes)\n",
    "    ax_label_overlay.axis('off')\n",
    "    for col_idx, t_idx in enumerate(timesteps_indices):\n",
    "        ax = fig.add_subplot(gs[3, col_idx + 1])\n",
    "        gt_frame = to_uint8_img(gt_highres_batch[b_idx:b_idx+1, t_idx])\n",
    "        pred_frame = to_uint8_img(highres_batch[b_idx:b_idx+1, t_idx])\n",
    "        \n",
    "        ax.imshow(gt_frame, alpha=0.7)\n",
    "        \n",
    "        gt_pos = extract_ball_position(gt_frame, threshold=0.7)\n",
    "        if gt_pos is not None:\n",
    "            y_gt, x_gt = gt_pos\n",
    "            circle_gt = plt.Circle((x_gt, y_gt), radius=5, color='red', \n",
    "                                   fill=False, linewidth=3, label='GT' if col_idx == 0 else '')\n",
    "            ax.add_patch(circle_gt)\n",
    "        \n",
    "        pred_pos = extract_ball_position(pred_frame, threshold=0.7)\n",
    "        if pred_pos is not None:\n",
    "            y_pred, x_pred = pred_pos\n",
    "            circle_pred = plt.Circle((x_pred, y_pred), radius=5, color='lime', \n",
    "                                     fill=False, linewidth=3, label='Ours' if col_idx == 0 else '')\n",
    "            ax.add_patch(circle_pred)\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    sample_name = os.path.splitext(os.path.basename(sample_paths[b_idx]))[0] if b_idx < len(sample_paths) else f'sample_{b_idx}'\n",
    "    save_path = os.path.join(FIGURE1_OUT_DIR, f'figure1_sample_{b_idx:02d}_{sample_name}.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f'Saved: {save_path}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(f'\\nFigure 1 saved to: {FIGURE1_OUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIGURE2_NUM_SAMPLES = 35\n",
    "FIGURE2_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "\n",
    "os.makedirs(FIGURE2_OUT_DIR, exist_ok=True)\n",
    "\n",
    "B = highres_batch.shape[0]\n",
    "T = highres_batch.shape[1]\n",
    "\n",
    "num_samples_to_plot = min(B, FIGURE2_NUM_SAMPLES)\n",
    "\n",
    "print(f\"Creating Figure 2 with {num_samples_to_plot} samples\")\n",
    "\n",
    "n_cols = 7\n",
    "n_rows = (num_samples_to_plot + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(35, 25), squeeze=False)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for plot_idx in range(num_samples_to_plot):\n",
    "    b_idx = plot_idx\n",
    "    \n",
    "    gt_trajectory = extract_trajectory(gt_highres_batch[b_idx:b_idx+1], threshold=0.7)\n",
    "    \n",
    "    pred_trajectory = extract_trajectory(highres_batch[b_idx:b_idx+1], threshold=0.7)\n",
    "    \n",
    "    ax = axes_flat[plot_idx]\n",
    "    \n",
    "    if len(gt_trajectory) > 0 and len(pred_trajectory) > 0:\n",
    "        gt_x = [pos[1] for pos in gt_trajectory]\n",
    "        gt_y = [pos[0] for pos in gt_trajectory]\n",
    "        pred_x = [pos[1] for pos in pred_trajectory]\n",
    "        pred_y = [pos[0] for pos in pred_trajectory]\n",
    "        \n",
    "        if 'cond_frames_list_to_process' in globals() and len(cond_frames_list_to_process) > 0:\n",
    "            cond_frames_used = cond_frames_list_to_process[0]\n",
    "        elif 'cond_frames_to_use' in globals():\n",
    "            cond_frames_used = cond_frames_to_use\n",
    "        else:\n",
    "            cond_frames_used = COND_FRAMES if isinstance(COND_FRAMES, int) else 5\n",
    "        cond_end_idx = min(cond_frames_used, len(gt_trajectory), len(pred_trajectory))\n",
    "        \n",
    "        ax.plot(gt_x[:cond_end_idx+1], gt_y[:cond_end_idx+1], \n",
    "                color='blue', linewidth=5, linestyle='-', \n",
    "                label='input ('+str(cond_end_idx)+' frames)', alpha=0.9, zorder=4)\n",
    "        ax.plot(gt_x[cond_end_idx:], gt_y[cond_end_idx:], \n",
    "                'r-.', linewidth=5, label='GT', alpha=0.9, zorder=2)\n",
    "        \n",
    "        ax.plot(pred_x[cond_end_idx:], pred_y[cond_end_idx:], \n",
    "                'g--', linewidth=5, label='Ours', alpha=0.9, dashes=(5, 3), zorder=2)\n",
    "        \n",
    "    \n",
    "    W, H = gt_highres_batch[0].shape[2:]\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(0, W-1)\n",
    "    ax.set_ylim(H-1, 0)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_facecolor('white')\n",
    "    ax.tick_params(labelsize=20)\n",
    "\n",
    "for idx in range(num_samples_to_plot, len(axes_flat)):\n",
    "    axes_flat[idx].axis('off')\n",
    "if 'cond_frames_list_to_process' in globals() and len(cond_frames_list_to_process) > 0:\n",
    "    cond_frames_for_legend = cond_frames_list_to_process[0]\n",
    "elif 'cond_frames_to_use' in globals():\n",
    "    cond_frames_for_legend = cond_frames_to_use\n",
    "else:\n",
    "    cond_frames_for_legend = COND_FRAMES if isinstance(COND_FRAMES, int) else 5\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', linewidth=3, linestyle='-', label=f'Input ({cond_frames_for_legend} frames)'),\n",
    "    Line2D([0], [0], color='r', linewidth=3, linestyle='-.', label='GT'),\n",
    "    Line2D([0], [0], color='g', linewidth=3, linestyle='--', label='Ours'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.02), \n",
    "           ncol=4, fontsize=30, frameon=True, fancybox=True, shadow=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.05)\n",
    "save_path = os.path.join(FIGURE2_OUT_DIR, 'figure2_trajectory_comparison.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'Saved: {save_path}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFigure 2 saved to: {FIGURE2_OUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(FIGURE3_OUT_DIR, exist_ok=True)\n",
    "\n",
    "if 'cond_frames_list_to_process' in globals():\n",
    "    cond_frames_to_use_for_fig3 = cond_frames_list_to_process\n",
    "    print(f\"Using cond_frames from cond_frames_list_to_process: {cond_frames_to_use_for_fig3}\")\n",
    "else:\n",
    "    cond_frames_to_use_for_fig3 = FIGURE3_COND_FRAMES_LIST\n",
    "    print(f\"Using cond_frames from FIGURE3_COND_FRAMES_LIST: {cond_frames_to_use_for_fig3}\")\n",
    "\n",
    "cond_frames_to_use_for_fig3 = cond_frames_to_use_for_fig3[:-1]\n",
    "print(f\"Using fixed sequence index: {cond_frames_to_use_for_fig3}\")\n",
    "\n",
    "if 'highres_gen_batches' not in globals():\n",
    "    raise RuntimeError(\"highres_gen_batches not found. Please run planner and refiner cells first.\")\n",
    "print(f\"Found highres_gen_batches with keys: {list(highres_gen_batches.keys())}\")\n",
    "\n",
    "gt_trajectory_full = extract_trajectory(gt_highres_batch[FIGURE3_SEQUENCE_INDEX:FIGURE3_SEQUENCE_INDEX+1], threshold=0.7)\n",
    "print(f\"GT trajectory length: {len(gt_trajectory_full)}\")\n",
    "\n",
    "cond_frames_results = {} \n",
    "for cond_frames_val in cond_frames_to_use_for_fig3:\n",
    "    print(f\"\\n--- Processing cond_frames={cond_frames_val} ---\")\n",
    "    \n",
    "    if 'highres_gen_batches' not in globals():\n",
    "        raise RuntimeError(f\"highres_gen_batches not found. Please run planner and refiner cells first.\")\n",
    "    \n",
    "    if cond_frames_val not in highres_gen_batches:\n",
    "        available_keys = list(highres_gen_batches.keys())\n",
    "        raise KeyError(\n",
    "            f\"cond_frames={cond_frames_val} not found in highres_gen_batches. \"\n",
    "            f\"Available keys: {available_keys}. \"\n",
    "            f\"Please run planner and refiner cells with cond_frames={cond_frames_val} first.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"  Using pre-generated result from highres_gen_batches\")\n",
    "    highres_pred = highres_gen_batches[cond_frames_val]\n",
    "    pred_trajectory = extract_trajectory(highres_pred[FIGURE3_SEQUENCE_INDEX:FIGURE3_SEQUENCE_INDEX+1], threshold=0.7)\n",
    "\n",
    "    input_trajectory = extract_trajectory(\n",
    "        gt_highres_batch[FIGURE3_SEQUENCE_INDEX:FIGURE3_SEQUENCE_INDEX+1, :cond_frames_val], \n",
    "        threshold=0.7\n",
    "    )\n",
    "    \n",
    "    cond_frames_results[cond_frames_val] = {\n",
    "        'gt': gt_trajectory_full,\n",
    "        'pred': pred_trajectory,\n",
    "        'input': input_trajectory\n",
    "    }\n",
    "    \n",
    "    print(f\"  Pred trajectory length: {len(pred_trajectory)}\")\n",
    "    print(f\"  Input trajectory length: {len(input_trajectory)}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(6*3, 6*2), squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for col_idx, cond_frames_val in enumerate(cond_frames_to_use_for_fig3):\n",
    "    ax = axes_flat[col_idx]\n",
    "    \n",
    "    results = cond_frames_results[cond_frames_val]\n",
    "    gt_traj = results['gt']\n",
    "    pred_traj = results['pred']\n",
    "    input_traj = results['input']\n",
    "    \n",
    "    gt_x = [pos[1] for pos in gt_traj]\n",
    "    gt_y = [pos[0] for pos in gt_traj]\n",
    "    pred_x = [pos[1] for pos in pred_traj]\n",
    "    pred_y = [pos[0] for pos in pred_traj]\n",
    "    input_x = [pos[1] for pos in input_traj]\n",
    "    input_y = [pos[0] for pos in input_traj]\n",
    "    \n",
    "    if len(input_x) > 0:\n",
    "        ax.plot(input_x, input_y, color='blue', linewidth=5, linestyle='-', \n",
    "               label='Input', alpha=0.9, zorder=4)\n",
    "    \n",
    "    if len(gt_x) > 0:\n",
    "        ax.plot(gt_x, gt_y, color='red', linewidth=5, linestyle='-', \n",
    "               label='GT', alpha=0.9, zorder=2)\n",
    "    \n",
    "    if len(pred_x) > 0:\n",
    "        ax.plot(pred_x, pred_y, color='green', linewidth=5, linestyle='--', \n",
    "               dashes=(5, 3), label='Predicted', alpha=0.9, zorder=3)\n",
    "    \n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f'cond frames={cond_frames_val}', fontsize=30, fontweight='bold')\n",
    "    ax.tick_params(labelsize=20)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', linewidth=3, linestyle='-', label='Input'),\n",
    "    Line2D([0], [0], color='red', linewidth=2.5, linestyle='-', label='GT'),\n",
    "    Line2D([0], [0], color='green', linewidth=2.5, linestyle='--', dashes=(5, 3), label='Predicted'),\n",
    "]\n",
    "legend_ax = axes_flat[-1]\n",
    "legend_ax.axis('off')\n",
    "\n",
    "legend_ax.legend(\n",
    "    handles=legend_elements,\n",
    "    loc='center',\n",
    "    shadow=True,\n",
    "    fontsize=30,\n",
    "    ncol=1\n",
    ")\n",
    "plt.tight_layout()\n",
    "save_path = os.path.join(FIGURE3_OUT_DIR, 'figure3_conditioning_effect.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'\\nSaved: {save_path}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFigure 3 saved to: {FIGURE3_OUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(FIGURE4_OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Creating Figure 4: Temporal Consistency Analysis (GT-deviation)\")\n",
    "print(f\"Using sequence index: {FIGURE4_SEQUENCE_INDEX}\")\n",
    "\n",
    "def compute_frame_diff_magnitude(video: torch.Tensor) -> np.ndarray:\n",
    "    if video.ndim == 5:  # [B, T, C, H, W]\n",
    "        video = video[0]  # use first batch\n",
    "\n",
    "    T = video.shape[0]\n",
    "    if T < 2:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "    # Î”_t = mean(|x_t - x_{t-1}|)\n",
    "    diffs = torch.mean(torch.abs(video[1:] - video[:-1]), dim=(1, 2, 3))  # [T-1]\n",
    "    return diffs.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "def compute_temporal_deviation(pred_video: torch.Tensor, gt_video: torch.Tensor) -> np.ndarray:\n",
    "    pred_d = compute_frame_diff_magnitude(pred_video)\n",
    "    gt_d = compute_frame_diff_magnitude(gt_video)\n",
    "\n",
    "    L = min(len(pred_d), len(gt_d))\n",
    "    if L == 0:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "    return np.abs(pred_d[:L] - gt_d[:L])\n",
    "\n",
    "def downsample_video_to_match(gt_video: torch.Tensor, ref_video: torch.Tensor) -> torch.Tensor:\n",
    "    assert gt_video.ndim == 5 and ref_video.ndim == 5\n",
    "    _, T, C, _, _ = gt_video.shape\n",
    "    _, T2, C2, h, w = ref_video.shape\n",
    "    assert T == T2 and C == C2, \"GT and ref must have same T and C to match temporal comparison.\"\n",
    "\n",
    "    # reshape to [T, C, H, W] for interpolate (treat T as batch)\n",
    "    x = gt_video[0]  # [T, C, H, W]\n",
    "    x_ds = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)  # [T, C, h, w]\n",
    "    return x_ds.unsqueeze(0)  # [1, T, C, h, w]\n",
    "\n",
    "idx = 3\n",
    "cond_frames = cond_frames_list_to_process[idx]\n",
    "cond_frames = COND_FRAMES if isinstance(COND_FRAMES, int) else 5\n",
    "\n",
    "print(f\"Using cond_frames: {cond_frames}\")\n",
    "\n",
    "# ---- GT (high-res) ----\n",
    "gt_high = gt_highres_batch[FIGURE4_SEQUENCE_INDEX:FIGURE4_SEQUENCE_INDEX+1]  # [1, T, C, H, W]\n",
    "\n",
    "# ---- Planner (low-res) ----\n",
    "if 'lowres_gen_batches' not in globals():\n",
    "    raise RuntimeError(\"lowres_gen_batches not found. Please run planner generation first.\")\n",
    "\n",
    "if cond_frames not in lowres_gen_batches:\n",
    "    available_keys = list(lowres_gen_batches.keys())\n",
    "    raise KeyError(f\"cond_frames={cond_frames} not found in lowres_gen_batches. Available: {available_keys}\")\n",
    "\n",
    "planner_low = lowres_gen_batches[cond_frames][FIGURE4_SEQUENCE_INDEX:FIGURE4_SEQUENCE_INDEX+1]  # [1, T, C, h, w]\n",
    "\n",
    "gt_low = downsample_video_to_match(gt_high, planner_low)  # [1, T, C, h, w]\n",
    "\n",
    "# ---- End-to-End (high-res) ----\n",
    "if 'highres_gen_batches' not in globals():\n",
    "    raise RuntimeError(\"highres_gen_batches not found. Please run end-to-end (high-res) generation first.\")\n",
    "\n",
    "if cond_frames not in highres_gen_batches:\n",
    "    available_keys = list(highres_gen_batches.keys())\n",
    "    raise KeyError(f\"cond_frames={cond_frames} not found in highres_gen_batches. Available: {available_keys}\")\n",
    "\n",
    "e2e_high = highres_gen_batches[cond_frames][FIGURE4_SEQUENCE_INDEX:FIGURE4_SEQUENCE_INDEX+1]  # [1, T, C, H, W]\n",
    "\n",
    "# ---- Compute temporal deviation curves ----\n",
    "planner_temporal_err = compute_temporal_deviation(planner_low, gt_low)\n",
    "print(f\"Planner temporal deviation shape: {planner_temporal_err.shape}\")\n",
    "\n",
    "e2e_temporal_err = compute_temporal_deviation(e2e_high, gt_high)\n",
    "print(f\"End-to-End temporal deviation shape: {e2e_temporal_err.shape}\")\n",
    "\n",
    "L = min(len(planner_temporal_err), len(e2e_temporal_err))\n",
    "frame_indices = np.arange(2, L + 2)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ax.plot(frame_indices, planner_temporal_err[:L],\n",
    "        linewidth=2.5, linestyle='-',\n",
    "        label='Planner (low-res)', alpha=0.9, zorder=2)\n",
    "\n",
    "ax.plot(frame_indices, e2e_temporal_err[:L],\n",
    "        linewidth=2.5, linestyle='--',\n",
    "        dashes=(5, 3), label='End-to-End (high-res)', alpha=0.9, zorder=1)\n",
    "\n",
    "# baseline\n",
    "ax.axhline(0.0, linewidth=2.0, linestyle='-',\n",
    "           label='GT (zero deviation)', alpha=0.8, zorder=3)\n",
    "\n",
    "ax.set_xlabel('Video Time Step (frame index)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(r'Temporal Deviation from GT  $|\\Delta x^{pred}-\\Delta x^{GT}|$', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Temporal Consistency Analysis (Deviation from GT Dynamics)', fontsize=16, fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(loc='best', fontsize=12, frameon=True, fancybox=True, shadow=True)\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(FIGURE4_OUT_DIR, 'figure4_temporal_consistency_deviation.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'\\nSaved: {save_path}')\n",
    "\n",
    "plt.show()\n",
    "print(f'\\nFigure 4 saved to: {FIGURE4_OUT_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(DATASET_FIG_OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Creating 3 independent Dataset Example Figures\")\n",
    "print(f\"Grid samples: {DATASET_FIG_NUM_SAMPLES_GRID}\")\n",
    "print(f\"Timeline sequences: {DATASET_FIG_NUM_SEQUENCES_TIMELINE}\")\n",
    "print(f\"Trajectory sequences: {DATASET_FIG_NUM_TRAJECTORIES}\")\n",
    "\n",
    "if 'gt_highres_batch' not in globals():\n",
    "    raise RuntimeError(\"gt_highres_batch not found. Please load GT data first.\")\n",
    "\n",
    "B = gt_highres_batch.shape[0]\n",
    "T = gt_highres_batch.shape[1]\n",
    "\n",
    "num_samples_available = min(B, DATASET_FIG_NUM_SAMPLES_GRID)\n",
    "num_timeline_available = min(B, DATASET_FIG_NUM_SEQUENCES_TIMELINE)\n",
    "num_traj_available = min(B, DATASET_FIG_NUM_TRAJECTORIES)\n",
    "\n",
    "print(f\"Available samples: {B}, using: grid={num_samples_available}, timeline={num_timeline_available}, trajectory={num_traj_available}\")\n",
    "\n",
    "print(\"\\n--- Creating Figure (A): Diverse Initial Conditions ---\")\n",
    "n_cols_grid = 3\n",
    "n_rows_grid = 2\n",
    "t_representative = 0\n",
    "\n",
    "fig_a = plt.figure(figsize=(25, 17))\n",
    "gs_a = fig_a.add_gridspec(n_rows_grid, n_cols_grid)\n",
    "\n",
    "for idx in range(num_samples_available):\n",
    "    row = idx // n_cols_grid\n",
    "    col = idx % n_cols_grid\n",
    "    if row < n_rows_grid:\n",
    "        ax_small = fig_a.add_subplot(gs_a[row, col])\n",
    "        frame = to_uint8_img(gt_highres_batch[idx:idx+1, t_representative])\n",
    "        ax_small.imshow(frame)\n",
    "        ax_small.axis('off')\n",
    "        ax_small.text(10, 15, f'S{idx+1}', fontsize=40, color='white', \n",
    "                     bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "        \n",
    "        if T > 1:\n",
    "            frame_t0_tensor = gt_highres_batch[idx:idx+1, 0]\n",
    "            frame_t1_tensor = gt_highres_batch[idx:idx+1, 1]\n",
    "            frame_t0_np = to_uint8_img(frame_t0_tensor)\n",
    "            frame_t1_np = to_uint8_img(frame_t1_tensor)\n",
    "            pos_t0 = extract_ball_position(frame_t0_np, threshold=0.7)\n",
    "            pos_t1 = extract_ball_position(frame_t1_np, threshold=0.7)\n",
    "            \n",
    "            if pos_t0 and pos_t1:\n",
    "                y0, x0 = pos_t0\n",
    "                y1, x1 = pos_t1\n",
    "                dx = x1 - x0\n",
    "                dy = y1 - y0\n",
    "                norm = np.sqrt(dx**2 + dy**2)\n",
    "                if norm > 1.0:\n",
    "                    dx_norm = dx / norm\n",
    "                    dy_norm = dy / norm\n",
    "                    arrow_len = 25\n",
    "                    start_x = x0\n",
    "                    start_y = y0\n",
    "                    end_x = x0 + dx_norm * arrow_len\n",
    "                    end_y = y0 + dy_norm * arrow_len\n",
    "                    ax_small.annotate('', \n",
    "                                        xy=(end_x, end_y),\n",
    "                                        xytext=(start_x, start_y),\n",
    "                                        arrowprops=dict(arrowstyle='->, head_length=1.5, head_width=1.5', color='yellow', lw=10, zorder=30))\n",
    "\n",
    "for idx in range(num_samples_available, n_rows_grid * n_cols_grid):\n",
    "    row = idx // n_cols_grid\n",
    "    col = idx % n_cols_grid\n",
    "    ax_empty = fig_a.add_subplot(gs_a[row, col])\n",
    "    ax_empty.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path_a = os.path.join(DATASET_FIG_OUT_DIR, 'dataset_figure_A_initial_conditions.png')\n",
    "plt.savefig(save_path_a, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'Saved: {save_path_a}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Figure (B): Temporal Evolution ======\n",
    "timesteps_to_plot = [t for t in DATASET_FIG_TIMESTEPS if t < T]\n",
    "timesteps_indices = [t for t in timesteps_to_plot]\n",
    "n_timesteps = len(timesteps_indices)\n",
    "n_timeline = num_timeline_available\n",
    "print(f\"Timeline sequences: {n_timeline}\")\n",
    "\n",
    "fig_b = plt.figure(figsize=(17, 10))\n",
    "gs_b = fig_b.add_gridspec(n_timeline, n_timesteps + 1,\n",
    "                         width_ratios=[0.15] + [1]*n_timesteps)\n",
    "\n",
    "for seq_idx in range(n_timeline):\n",
    "    ax_label = fig_b.add_subplot(gs_b[seq_idx, 0])\n",
    "    ax_label.text(0.5, 0.5, f'S{seq_idx+1}', fontsize=20, fontweight='bold',\n",
    "                  ha='center', va='center', transform=ax_label.transAxes)\n",
    "    ax_label.axis('off')\n",
    "    \n",
    "    for col_idx, t_idx in enumerate(timesteps_indices):\n",
    "        ax_frame = fig_b.add_subplot(gs_b[seq_idx, col_idx + 1])\n",
    "        frame = to_uint8_img(gt_highres_batch[seq_idx:seq_idx+1, t_idx])\n",
    "        ax_frame.imshow(frame)\n",
    "        ax_frame.axis('off')\n",
    "\n",
    "        if seq_idx == 0:\n",
    "            ax_frame.set_title(f't={t_idx}', fontsize=25, pad=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path_b = os.path.join(DATASET_FIG_OUT_DIR, 'dataset_figure_B_temporal_evolution.png')\n",
    "plt.savefig(save_path_b, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'Saved: {save_path_b}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_trajectory_error_over_time(\n",
    "    pred_video: torch.Tensor,\n",
    "    gt_video: torch.Tensor,\n",
    "    threshold: float = 0.7\n",
    ") -> np.ndarray:\n",
    "    def to_uint8(tensor):\n",
    "        tensor = tensor.detach().float().clamp(-1, 1)\n",
    "        tensor = (tensor + 1) * 0.5\n",
    "        tensor = (tensor * 255.0).round().to(torch.uint8)\n",
    "        return tensor\n",
    "    \n",
    "    if pred_video.ndim == 4:\n",
    "        pred_video = pred_video.unsqueeze(0)\n",
    "        gt_video = gt_video.unsqueeze(0)\n",
    "    \n",
    "    B, T, C, H, W = pred_video.shape\n",
    "    errors = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        pred_frame = to_uint8(pred_video[0, t]).permute(1, 2, 0).cpu().numpy()  # [H, W, 3]\n",
    "        gt_frame = to_uint8(gt_video[0, t]).permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        pred_pos = extract_ball_position(pred_frame, threshold)\n",
    "        gt_pos = extract_ball_position(gt_frame, threshold)\n",
    "        \n",
    "        if pred_pos is not None and gt_pos is not None:\n",
    "            error = np.sqrt((pred_pos[0] - gt_pos[0])**2 + (pred_pos[1] - gt_pos[1])**2)\n",
    "            errors.append(error)\n",
    "        else:\n",
    "            if len(errors) > 0:\n",
    "                errors.append(errors[-1])\n",
    "            else:\n",
    "                errors.append(0.0)\n",
    "    \n",
    "    return np.array(errors)\n",
    "\n",
    "TRAJ_ERROR_OUT_DIR = os.path.join(OUT_DIR, 'figures')\n",
    "os.makedirs(TRAJ_ERROR_OUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAJ_ERROR_SEQUENCE_INDEX = 0\n",
    "\n",
    "print(\"Creating Trajectory Error Analysis Figure\")\n",
    "print(f\"Sequence index: {TRAJ_ERROR_SEQUENCE_INDEX}\")\n",
    "\n",
    "if 'gt_highres_batch' not in globals():\n",
    "    raise RuntimeError(\"gt_highres_batch not found. Please load GT data first.\")\n",
    "\n",
    "if 'highres_gen_batches' not in globals():\n",
    "    raise RuntimeError(\"highres_gen_batches not found. Please run planner and refiner cells first.\")\n",
    "\n",
    "if 'cond_frames_list_to_process' not in globals():\n",
    "    if 'COND_FRAMES_LIST' in globals():\n",
    "        cond_frames_list_to_process = COND_FRAMES_LIST\n",
    "    else:\n",
    "        cond_frames_list_to_process = [1, 3, 5, 10]\n",
    "    print(f\"Using default cond_frames_list: {cond_frames_list_to_process}\")\n",
    "else:\n",
    "    print(f\"Using cond_frames_list_to_process: {cond_frames_list_to_process}\")\n",
    "\n",
    "gt_video = gt_highres_batch[TRAJ_ERROR_SEQUENCE_INDEX:TRAJ_ERROR_SEQUENCE_INDEX+1]  # [1, T, C, H, W]\n",
    "\n",
    "trajectory_errors = {}  # {cond_frames: error_array}\n",
    "\n",
    "for cond_frames_val in cond_frames_list_to_process[:-1]:\n",
    "    if cond_frames_val not in highres_gen_batches:\n",
    "        print(f\"WARNING: cond_frames={cond_frames_val} not found in highres_gen_batches, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    pred_video = highres_gen_batches[cond_frames_val][TRAJ_ERROR_SEQUENCE_INDEX:TRAJ_ERROR_SEQUENCE_INDEX+1]\n",
    "    \n",
    "    T_min = min(pred_video.shape[1], gt_video.shape[1])\n",
    "    pred_video = pred_video[:, :T_min]\n",
    "    gt_video_aligned = gt_video[:, :T_min]\n",
    "    \n",
    "    errors = compute_trajectory_error_over_time(pred_video, gt_video_aligned, threshold=0.7)\n",
    "    trajectory_errors[cond_frames_val] = errors\n",
    "    print(f\"cond_frames={cond_frames_val}: error shape={errors.shape}, mean={np.mean(errors):.2f}, max={np.max(errors):.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "strong_colors = [\n",
    "    '#FF0000',\n",
    "    '#0000FF',\n",
    "    '#00AA00',\n",
    "    '#FF6600',\n",
    "    '#9900CC',\n",
    "    '#CC0000',\n",
    "    '#0066CC',\n",
    "    '#000000',\n",
    "]\n",
    "\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--', '-.', ':']\n",
    "\n",
    "markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h']\n",
    "\n",
    "sorted_keys = sorted(trajectory_errors.keys())\n",
    "color_map = {}\n",
    "style_map = {}\n",
    "marker_map = {}\n",
    "\n",
    "for i, cond_frames_val in enumerate(sorted_keys):\n",
    "    color_map[cond_frames_val] = strong_colors[i % len(strong_colors)]\n",
    "    style_map[cond_frames_val] = line_styles[i % len(line_styles)]\n",
    "    marker_map[cond_frames_val] = markers[i % len(markers)]\n",
    "\n",
    "for cond_frames_val in sorted_keys:\n",
    "    errors = trajectory_errors[cond_frames_val]\n",
    "    time_steps = np.arange(len(errors))\n",
    "    color = color_map[cond_frames_val]\n",
    "    linestyle = style_map[cond_frames_val]\n",
    "    marker = marker_map[cond_frames_val]\n",
    "    \n",
    "    marker_every = max(1, len(time_steps) // 20)\n",
    "    \n",
    "    ax.plot(time_steps, errors, \n",
    "            linewidth=4.5,\n",
    "            linestyle=linestyle,\n",
    "            marker=marker,\n",
    "            markevery=marker_every,\n",
    "            markersize=10,\n",
    "            label=f'cond_frames={cond_frames_val}',\n",
    "            color=color,\n",
    "            alpha=1.0,\n",
    "            zorder=2,\n",
    "            markeredgecolor='white',\n",
    "            markeredgewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Time (frame index)', fontsize=20, fontweight='bold')\n",
    "ax.set_ylabel('Trajectory Error (pixels)', fontsize=20, fontweight='bold')\n",
    "ax.set_title('Trajectory Error Over Time for Different Conditioning Frames', \n",
    "             fontsize=22, fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.5, linestyle='--', linewidth=1.5, zorder=0)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=16, width=2, length=6)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "\n",
    "ax.legend(loc='best', fontsize=16, frameon=True, fancybox=True, shadow=True, \n",
    "          ncol=2, framealpha=0.95, edgecolor='gray', columnspacing=1.2,\n",
    "          handlelength=3.0, handletextpad=0.8, markerscale=1.5)\n",
    "\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(TRAJ_ERROR_OUT_DIR, 'trajectory_error_multiple_cond_frames.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'\\nSaved: {save_path}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nTrajectory Error Analysis Figure saved to: {TRAJ_ERROR_OUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating Figure (C): Trajectory Patterns ---\")\n",
    "n_cols_traj = 3\n",
    "n_rows_traj = 2\n",
    "\n",
    "fig_c = plt.figure(figsize=(6*n_cols_traj, 6*n_rows_traj))\n",
    "gs_c = fig_c.add_gridspec(n_rows_traj, n_cols_traj, hspace=0.3, wspace=0.2)\n",
    "\n",
    "for idx in range(num_traj_available):\n",
    "    row = idx // n_cols_traj\n",
    "    col = idx % n_cols_traj\n",
    "    ax_traj = fig_c.add_subplot(gs_c[row, col])\n",
    "    \n",
    "    gt_trajectory = extract_trajectory(gt_highres_batch[idx:idx+1], threshold=0.7)\n",
    "    \n",
    "    if len(gt_trajectory) > 0:\n",
    "        gt_x = [pos[1] for pos in gt_trajectory]\n",
    "        gt_y = [pos[0] for pos in gt_trajectory]\n",
    "        ax_traj.plot(gt_x, gt_y, 'b-', linewidth=5, alpha=0.8, zorder=2)\n",
    "        \n",
    "        if len(gt_x) > 0:\n",
    "            ax_traj.scatter([gt_x[0]], [gt_y[0]], c='green', s=200, marker='o', \n",
    "                           zorder=3, label='Start', edgecolors='white', linewidths=1)\n",
    "            ax_traj.scatter([gt_x[-1]], [gt_y[-1]], c='red', s=200, marker='s', \n",
    "                           zorder=3, label='End', edgecolors='white', linewidths=1)\n",
    "    \n",
    "    W, H = gt_highres_batch[0].shape[2:]\n",
    "    ax_traj.set_aspect('equal')\n",
    "    ax_traj.set_xlim(0, W-1)\n",
    "    ax_traj.set_ylim(H-1, 0)\n",
    "    ax_traj.set_title(f'S{idx+1}', fontsize=30, fontweight='bold')\n",
    "    ax_traj.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax_traj.set_facecolor('white')\n",
    "    ax_traj.tick_params(labelsize=20)\n",
    "\n",
    "for idx in range(num_traj_available, n_rows_traj * n_cols_traj):\n",
    "    row = idx // n_cols_traj\n",
    "    col = idx % n_cols_traj\n",
    "    ax_empty = fig_c.add_subplot(gs_c[row, col])\n",
    "    ax_empty.axis('off')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        color='blue',\n",
    "        linewidth=5,\n",
    "        label='Trajectory'\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker='o',\n",
    "        color='green',\n",
    "        linestyle='None',\n",
    "        markersize=30,\n",
    "        markeredgecolor='white',\n",
    "        markeredgewidth=1.5,\n",
    "        label='Start'\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker='s',\n",
    "        color='red',\n",
    "        linestyle='None',\n",
    "        markersize=30,\n",
    "        markeredgecolor='white',\n",
    "        markeredgewidth=1.5,\n",
    "        label='End'\n",
    "    ),\n",
    "]\n",
    "\n",
    "fig_c.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.02), \n",
    "           ncol=4, fontsize=30, frameon=True, fancybox=True, shadow=True)\n",
    "\n",
    "save_path_c = os.path.join(DATASET_FIG_OUT_DIR, 'dataset_figure_C_trajectory_patterns.png')\n",
    "plt.savefig(save_path_c, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'Saved: {save_path_c}')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nAll 3 Dataset Example Figures saved to: {DATASET_FIG_OUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(FIGURE4_OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Creating Figure 4: Temporal Consistency Analysis (GT-deviation)\")\n",
    "print(f\"Using sequence index: {FIGURE4_SEQUENCE_INDEX}\")\n",
    "\n",
    "def compute_frame_diff_magnitude(video: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute mean absolute frame-to-frame difference magnitude.\n",
    "    video: [B, T, C, H, W] or [T, C, H, W], value range [-1, 1]\n",
    "    returns: [T-1] numpy array\n",
    "    \"\"\"\n",
    "    if video.ndim == 5:  # [B, T, C, H, W]\n",
    "        video = video[0]  # use first batch\n",
    "\n",
    "    T = video.shape[0]\n",
    "    if T < 2:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "    # Î”_t = mean(|x_t - x_{t-1}|)\n",
    "    diffs = torch.mean(torch.abs(video[1:] - video[:-1]), dim=(1, 2, 3))  # [T-1]\n",
    "    return diffs.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "def compute_temporal_deviation(pred_video: torch.Tensor, gt_video: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Temporal deviation from GT dynamics:\n",
    "      err_t = | mean(|pred_t - pred_{t-1}|) - mean(|gt_t - gt_{t-1}|) |\n",
    "    returns: [min(T_pred, T_gt)-1] numpy array\n",
    "    \"\"\"\n",
    "    pred_d = compute_frame_diff_magnitude(pred_video)\n",
    "    gt_d = compute_frame_diff_magnitude(gt_video)\n",
    "\n",
    "    L = min(len(pred_d), len(gt_d))\n",
    "    if L == 0:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "    return np.abs(pred_d[:L] - gt_d[:L])\n",
    "\n",
    "def downsample_video_to_match(gt_video: torch.Tensor, ref_video: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Downsample gt_video spatially to match ref_video resolution.\n",
    "    gt_video:  [1, T, C, H, W] (high-res GT)\n",
    "    ref_video: [1, T, C, h, w] (e.g., low-res planner output)\n",
    "    returns:   [1, T, C, h, w]\n",
    "    \"\"\"\n",
    "    assert gt_video.ndim == 5 and ref_video.ndim == 5\n",
    "    _, T, C, _, _ = gt_video.shape\n",
    "    _, T2, C2, h, w = ref_video.shape\n",
    "    assert T == T2 and C == C2, \"GT and ref must have same T and C to match temporal comparison.\"\n",
    "\n",
    "    # reshape to [T, C, H, W] for interpolate (treat T as batch)\n",
    "    x = gt_video[0]  # [T, C, H, W]\n",
    "    x_ds = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)  # [T, C, h, w]\n",
    "    return x_ds.unsqueeze(0)  # [1, T, C, h, w]\n",
    "\n",
    "idx = 3\n",
    "cond_frames = cond_frames_list_to_process[idx]\n",
    "cond_frames = COND_FRAMES if isinstance(COND_FRAMES, int) else 5\n",
    "\n",
    "print(f\"Using cond_frames: {cond_frames}\")\n",
    "\n",
    "# ---- GT (high-res) ----\n",
    "gt_high = gt_highres_batch[FIGURE4_SEQUENCE_INDEX:FIGURE4_SEQUENCE_INDEX+1]  # [1, T, C, H, W]\n",
    "\n",
    "# ---- Planner (low-res) ----\n",
    "if 'lowres_gen_batches' not in globals():\n",
    "    raise RuntimeError(\"lowres_gen_batches not found. Please run planner generation first.\")\n",
    "\n",
    "if cond_frames not in lowres_gen_batches:\n",
    "    available_keys = list(lowres_gen_batches.keys())\n",
    "    raise KeyError(f\"cond_frames={cond_frames} not found in lowres_gen_batches. Available: {available_keys}\")\n",
    "\n",
    "planner_low = lowres_gen_batches[cond_frames][FIGURE4_SEQUENCE_INDEX:FIGURE4_SEQUENCE_INDEX+1]  # [1, T, C, h, w]\n",
    "\n",
    "gt_low = downsample_video_to_match(gt_high, planner_low)  # [1, T, C, h, w]\n",
    "\n",
    "# ---- End-to-End (high-res) ----\n",
    "if 'highres_gen_batches' not in globals():\n",
    "    raise RuntimeError(\"highres_gen_batches not found. Please run end-to-end (high-res) generation first.\")\n",
    "\n",
    "if cond_frames not in highres_gen_batches:\n",
    "    available_keys = list(highres_gen_batches.keys())\n",
    "    raise KeyError(f\"cond_frames={cond_frames} not found in highres_gen_batches. Available: {available_keys}\")\n",
    "\n",
    "e2e_high = highres_gen_batches[cond_frames][FIGURE4_SEQUENCE_INDEX:FIGURE4_SEQUENCE_INDEX+1]  # [1, T, C, H, W]\n",
    "\n",
    "# ---- Compute temporal deviation curves ----\n",
    "planner_temporal_err = compute_temporal_deviation(planner_low, gt_low)\n",
    "print(f\"Planner temporal deviation shape: {planner_temporal_err.shape}\")\n",
    "\n",
    "e2e_temporal_err = compute_temporal_deviation(e2e_high, gt_high)\n",
    "print(f\"End-to-End temporal deviation shape: {e2e_temporal_err.shape}\")\n",
    "\n",
    "L = min(len(planner_temporal_err), len(e2e_temporal_err))\n",
    "frame_indices = np.arange(2, L + 2)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ax.plot(frame_indices, planner_temporal_err[:L],\n",
    "        linewidth=2.5, linestyle='-',\n",
    "        label='Planner (low-res)', alpha=0.9, zorder=2)\n",
    "\n",
    "ax.plot(frame_indices, e2e_temporal_err[:L],\n",
    "        linewidth=2.5, linestyle='--',\n",
    "        dashes=(5, 3), label='End-to-End (high-res)', alpha=0.9, zorder=1)\n",
    "\n",
    "# baseline\n",
    "ax.axhline(0.0, linewidth=2.0, linestyle='-',\n",
    "           label='GT (zero deviation)', alpha=0.8, zorder=3)\n",
    "\n",
    "ax.set_xlabel('Video Time Step (frame index)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(r'Temporal Deviation from GT  $|\\Delta x^{pred}-\\Delta x^{GT}|$', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Temporal Consistency Analysis (Deviation from GT Dynamics)', fontsize=16, fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(loc='best', fontsize=12, frameon=True, fancybox=True, shadow=True)\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(FIGURE4_OUT_DIR, 'figure4_temporal_consistency_deviation.png')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f'\\nSaved: {save_path}')\n",
    "\n",
    "plt.show()\n",
    "print(f'\\nFigure 4 saved to: {FIGURE4_OUT_DIR}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
